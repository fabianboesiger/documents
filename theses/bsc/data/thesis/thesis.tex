% !TEX program = xelatex
% !BIB program = biber

\documentclass[11pt]{article}

\usepackage{biblatex}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{caption}

\setmonofont[
  Contextuals={Alternate},
  Scale=0.8
]{Fira Code}

\addbibresource{viper.bib}
\addbibresource{silicon.bib}
\addbibresource{patterns.bib}
\addbibresource{intellij.bib}
\addbibresource{caffeine.bib}
\addbibresource{smallfoot.bib}
\addbibresource{macros.bib}
\addbibresource{vercors.bib}
\addbibresource{nagini.bib}
\addbibresource{gobra.bib}
\addbibresource{prusti.bib}
\addbibresource{vyper.bib}

\parskip=1em
\parindent=0pt

\definecolor{codegrey}{rgb}{0.9,0.9,0.9}
\definecolor{commentgrey}{rgb}{0.5,0.5,0.5}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily,
    keywordstyle=\bfseries,
    commentstyle=\color{commentgrey},
    %backgroundcolor=\color{codegrey},
    frame=single,
    numbers=left,
}
\lstset{style=mystyle}

\DeclareMathOperator{\perm}{\mathbin{\#}}

\begin{document}

    \pagenumbering{gobble}
    \begin{titlepage}
        \begin{center}
            \textit{Bachelor's Thesis}\\
            \vspace{0.5cm}
            \textbf{\Huge Performance Improvements of a Program Verifier}\\
            \vfill   
            Fabian Bösiger\\
            Supervised by Dr. Malte Schwerhoff\\
            \vspace{0.2cm}
            Programming Methodology Group\\
            Department of Computer Science\\
            ETH Zürich\\
            \vspace{0.2cm}
            \today
        \end{center}
    \end{titlepage}
    \newpage

    \begin{abstract}
        \parskip=1em
        \parindent=0pt
        \noindent

        This thesis explores ways to improve performance of
        the Silicon program verification backend for the Viper
        verification infrastructure.
        
        In the first part,
        we explore the concept of applying the flyweight pattern on ASTs.
        Applying the flyweight pattern avoids multiple instances of structurally equal nodes
        existing at the same time. This allows us to replace structural and recursive
        equality checks with reference equality checks, with the goal of improving performance of equality checks.

        In the second part, we introduce more sophisticated ways to join symbolic execution paths in
        Silicon, after having branched on conditional expressions, implications and if-statements.
    \end{abstract}
    \newpage

    \renewcommand{\abstractname}{Acknowledgements}
    \begin{abstract}
        \parskip=1em
        \parindent=0pt
        \noindent

        I would like to thank my supervisor, Malte Schwerhoff, who provided
        me with the opportunity to write this thesis. The time
        and effort he expended to help and advice me was highly appreciated.

        Additionally, I'd like to thank Peter Müller and his group for sparking interest and providing insight
        into topics such as these.

        Furthermore, I'd like to express my gratitude towards my family for providing
        me with a very pleasant home office environment.
    \end{abstract}
    \newpage

    \setcounter{tocdepth}{2}
    \tableofcontents
    \newpage

    \pagenumbering{arabic}

    \section{Introduction}

    Viper \cite{viper} is a verification infrastructure on top of which verification tools
    for different programming languages can be built. Silicon \cite{silicon} is a backend for Viper,
    which is based on Smallfoot-style \cite{smallfoot} symbolic execution. 
    To advance program verification in practice,
    fast verification is crucial as it provides a more streamlined experience for developers.
    This is the reason why one of Silicon's stated goals is performance:

    \begin{quote} 
        ``The verifier should enable an IDE-like experience: it should be
        sufficiently fast such that users can continuously work on verifying
        programs [...]'' \cite{silicon}
    \end{quote}

    In this thesis, we explore two different approaches to improving performance of Silicon.

    \subsection*{First Approach}

    Silicon internally uses abstract syntax trees (AST) to represent
    the structure of a program as a tree data structure. As with any other tree structure,
    ASTs can be traversed, searched, transformed and so forth. During such operations,
    subtrees within the AST are potentially checked for equality many times.  Moreover, equality checks
    also occur in operations on collections of AST subtrees, for example in finding
    a specific subtree, which may add additional performance overhead.

    Equality checks can't easily be avoided, but they can be implemented in a more performant way.
    Silicon's AST nodes are called terms, which represent different program operations.
    \texttt{Plus(IntLiteral(1), IntLiteral(2))} represents the program code $1 + 2$. \texttt{Plus}, \texttt{IntLiteral(1)}
    and \texttt{IntLiteral(2)} are all terms.
    Currently in Silicon, new term instances are created independently of already existing ones, which potentially leads 
    to the coexistence of multiple structurally equal term instances. Subterm equality is checked in a
    structural and recursive manner. In part \ref{part-1} of working towards a potential improvement in performance,
    we explore the concept of applying the flyweight pattern \cite{patterns} on AST terms to only ever have
    one instance of some term structure, thus avoiding the need for structural and recursive equality checks.

    \subsection*{Second Approach}

    For verifying a program, Silicon uses the symbolic execution approach, where the program is interpreted,
    and a symbolic state keeps track of all possible program states at the current point of execution, for all
    possible input values of the program. When encountering certain expressions or statements, for example an
    if-statement, symbolic execution branches with the assumptions of the corresponding program path.

    Silicon currently only joins these branches for some simple cases. In other cases,
    branches aren't joined, which results in all statements later down the verification path
    being evaluated essentially twice, but with different assumptions in each branch. Both of these
    verification paths may branch again, potentially leading to exponential growth in the number of branches.
    In an effort to improve performance, part \ref{part-2} of this thesis focuses on implementing 
    joining of execution paths or more complex cases, which ultimately leads to fewer active branches.

    \part{Flyweight ASTs;\newline A Study in Applied Laziness} \label{part-1}
    \begin{center}
        \vspace{3cm}
        \includegraphics[width=0.7\textwidth]{fly}
    \end{center}

    \newpage
    \section{Approach}

    \subsection{Implementation of Flyweight ASTs}

    Currently in Silicon, new term instances are created independently of already existing ones, which potentially leads 
    to the coexistence of multiple structurally equal term instances. Subterm equality is checked in a
    structural and recursive manner.
    However the AST used in Silicon is immutable, so the flyweight pattern \cite{patterns}
    can be applied on AST terms.
    To do this, a pool of term instances is maintained. Whenever a term
    is to be created, the components of this new term is compared with the
    pool of existing terms. If a term with the same components already exists,
    a reference to the existing term is returned and
    the creation of a new instance is avoided. Otherwise, a new term is created and added to
    the pool.

    This gives the guarantee that there are no two instances of the
    same term in our pool, meaning every two structurally equal terms point to the same
    underlying object in memory. Comparing terms for structural equality then boils
    down to a cheap reference equality check, and recursive equality checks can be avoided,
    at the cost of increased overhead at the creation of a term instance due to the flyweight pattern.

    \subsection{Automate Boilerplate Generation Using Macros} \label{boilerplate-generation-using-macros}

    Silicon's AST representation of the Viper language consists of nearly 100 different terms,
    all with boilerplate implementations for different operations. For example,
    because Silicon usually doesn't use case classes for its terms,
    each term defines it's own \texttt{unapply} method.
    Our changes adds additional boilerplate code by implementing
    the flyweight pattern for each term as seen in listing \ref{implementation-flyweight-pattern}.

    Our ASTs shouldn't only be flyweight in the sense of the implementation pattern, but also
    regarding development time and effort.
    The Viper infrastructure is written in the Scala programming language, which 
    provides seamless interoperability with Java and has support
    for metaprogramming using macros. \cite{macros} This allows us to avoid boilerplate
    code and instead automatically generate it using Scala's support for macro annotations.
    Additional benefits of using macro annotations include improvements in code
    readability and maintainability. Experimenting with code changes will become a matter
    of editing a single macro instead of editing each term individually.
    Terms which may be added in the future are also easier to implement.

    \newpage
    \section{Implementation}

    \subsection{Implementation of the Flyweight Pattern} \label{implementation-flyweight-pattern}

    As the flyweight pattern for Silicon's terms is implemented in Scala,
    we assume general familiarity with the language. 
    The implementation works as follows:

    \begin{enumerate}
        \item The constructor of a term is made private (line 1) so that new term instances can't be created via
            the \texttt{new} keyword, but only via the \texttt{apply} method (line 9), which acts as the
            term factory and does the pool lookups.
        \item For every term, we create a map which maps the components of the term to the term itself (line 7).
            This allows us to later look up whether a structurally equal term was created already (line 10).
        \item In the \texttt{apply} method, we check the pool for structurally equal instances (line 10),
            and if one exists, we return it and thus avoid creating a new instance of the
            same term (line 20).
        \item If no structurally equal instance exists, we create a new instance via the \texttt{new}
            keyword, add it to the pool and return it (line 14, 15, 16).
    \end{enumerate}

    As an example, the implementation of the flyweight pattern for the \texttt{Plus} term is shown here:

    \begin{lstlisting}[language=Scala, caption={Implementation of the flyweight pattern.}, label={lst:flyweight-pattern}]
class Plus private (val p0: Term, val p1: Term) {
    // ...
}

object Plus extends ((Term, Term) => Term) {
    // Maps fields of the term to the term instance itself.
    var pool = new Map[(Term, Term), Term]

    def apply(e0: Term, e1: Term): Term = {
        pool.get((e0, e1)) match {
            // If no structurally equal term exists,
            // create a new one.
            case None => 
                val term = new Plus(e0, e1)
                pool.addOne((e0, e1), term)
                term
            // If a structurally equal term exists,
            // return a reference to it instead.
            case Some(term) => 
                term
        }
    }

    // ...
}       
    \end{lstlisting}

    \subsection{A Macro Annotation for Code Generation}

    The Viper infrastructure is written in the Scala programming language, which 
    has support for metaprogramming using macros. Silicon's different AST term classes are 
    an ideal target for static code generation, as they inherently share
    many similarities with each other, their code is structurally equivalent,
    but differ in type and arity.
    To address the problem of boilerplate
    code described in section \ref{boilerplate-generation-using-macros}, we implement
    a macro annotation that automatically generates required code.

    The code for the flyweight macro annotation exists as a subproject within Silicon. Each term can be
    annotated with \texttt{@flyweight}, which invokes the macro at compile time and 
    rewrites the term in the following way:

    \begin{enumerate}
        \item If an \texttt{apply} method is already defined, rename it to \texttt{\_apply}.
            The already defined \texttt{apply} method can't be discarded because it potentially
            defines additional operations required on creation of a term.
        \item Define a new \texttt{apply} method that introduces the flyweight pattern
            as discussed in section \ref{implementation-flyweight-pattern}.
            If a new term instance has to be created, either use  the previously 
            defined \texttt{\_apply} method
            if it exists, else simply create an instance using the \texttt{new} keyword.
        \item Generate a suitable \texttt{unapply} method.
        \item Generate a \texttt{copy} method that calls \texttt{apply} instead of creating
            instances via \texttt{new} such that the flyweight pattern can't be bypassed
            when copying a term.
        \item Override \texttt{hashCode} to use Java's \texttt{System.identityHashCode}.
    \end{enumerate}

    This process of rewriting terms using macros happens on every term
    annotated with \texttt{@flyweight}, and can in nicely illustrated by an example
    that considers the program input in listing \ref{lst:macro-input}, and output in listing \ref{lst:macro-output} of our macro:

    \begin{lstlisting}[language=Scala, caption={Input code annotated with the macro.}, label={lst:macro-input}]
@flyweight
class Plus(val p0: Term, val p1: Term)
    extends ArithmeticTerm
{
    override val op = "+"
}

object Plus extends ((Term, Term) => Term) {
    def apply(e0: Term, e1: Term): Term = (e0, e1) match {
        case (t0, Zero) => t0
        case (Zero, t1) => t1
        case (IntLiteral(n0), IntLiteral(n1)) => IntLiteral(n0 + n1)
        case _ => new Plus(e0, e1)
    }
}        
    \end{lstlisting}

    \begin{lstlisting}[language=Scala, caption={Output code generated by our macro.}, label={lst:macro-output}]
class Plus private (val p0: Term, val p1: Term)
    // Superclasses and implemented traits are preserved from input.
    extends ArithmeticTerm
{
    // Override hashCode.
    override val hashCode = System.identityHashCode(this)

    // Generated copy method which uses the generated apply method.
    def copy(p0: Term = p0, p1: Term = p1) = Plus(p0, p1)

    // Preserved from input.
    override val op = "+"
}

object Plus extends ((Term, Term) => Term) {
    var pool = new Map[(Term, Term), Term]

    // Define new apply method which uses the flyweight pattern.
    def apply(e0: Term, e1: Term): Term = {
        pool.get((e0, e1)) match {
            case None => 
                val term = Plus._apply(e0, e1)
                pool.addOne((e0, e1), term)
                term
            case Some(term) => 
                term
        }
    }

    // Generated unapply method.
    def unapply(t: Plus) =
        Some((t.p0, t.p1))

    // Renamed existing apply method to _apply.
    // AST simplifications implemented are thus preserved.
    def _apply(e0: Term, e1: Term): Term = (e0, e1) match {
        case (t0, IntLiteral(0)) => t0
        case (IntLiteral(0), t1) => t1
        case (IntLiteral(n0), IntLiteral(n1)) => IntLiteral(n0 + n1)
        case _ => new Plus(e0, e1)
    }
}       
    \end{lstlisting}

    %\subsubsection{Classes or Case Classes}

    %\subsubsection{Macro Annotations on Nodes}

    %\subsubsection{Clear Pools after each File}

    %\subsubsection{AST Reduction for Builtin Equals}

    %\subsubsection{Equality Defining Members}

    %\subsection{IntelliJ Support}

    \subsection{Flyweight Macro Support for IntelliJ} \label{macro-support-intellij}

    The Viper infrastructure is written in the Scala programming language.
    Scala has support for metaprogramming using macros, which provide
    a nice and easy way for metaprogramming and
    are regularly used. For a nice programming experience
    using macros, IDE support should ideally be provided. In this case, we use the IntelliJ IDE.
    However, coding assistance for Scala macros is not supported natively by the IntelliJ IDE, 
    as it is difficult for IDE's to provide proper syntax highlighting.:

    \begin{quote}
        ``Since IntelliJ IDEA’s coding assistance is based on static code analysis,
        the IDE is not aware of AST changes, and can’t provide appropriate code
        completion and inspections for the generated code.'' \cite{intellij}
    \end{quote}

    In the example of our flyweight macro, the IDE is not aware that
    the method \texttt{apply} is generated and exists in the by our macro
    expanded code. The IDE thus reports an error that the method \texttt{apply}
    doesn't exist
    wherever a term is applied, despite \texttt{apply} existing in the expanded code, as illustrated in figure \ref{fig:ide-problem}.

    To fix this issue for the IntelliJ IDE,
    we provide a plugin which
    can be easily installed in IntelliJ, and fixes the highlighting
    issues for the flyweight macro. The plugin is hard-coded to
    make the IDE aware of code changes introduced by the flyweight
    macro, as seen in figure \ref{fig:ide-fixed}.
    
    \begin{figure}[H]
        \begin{center}
            \includegraphics[width=0.8\textwidth]{ide-problem.png}
            \caption{The IDE is not aware of code changes done by our macro. This leads to incorrect error reporting.}
            \label{fig:ide-problem}
        \end{center}   
    \end{figure}

    \begin{figure}[H]
        \begin{center}
            \includegraphics[width=0.8\textwidth]{ide-fixed.png}
            \caption{The IDE is now aware of code changes done by the flyweight macro, which fixes error reporting.}
            \label{fig:ide-fixed}
        \end{center}   
    \end{figure}

    \subsubsection{Towards Full Scala Macro Support for IntelliJ} \label{full-scala-macro-support}

    The plugin discussed in section \ref{macro-support-intellij} is hard-coded to only support the flyweight macro.
    Changes in the flyweight macro require the programmer to manually modify
    the plugin.
    To encourage more experimentation using macros, it is of advantage
    to have a plugin which supports macros that may be modified, for example
    by generating additional methods.

    To support this kind of more dynamic plugin, the macro is modified to
    dump all generated method signatures into a XML configuration file whenever
    it is run by the compiler.
    This configuration file is then read by the IntelliJ plugin. The plugin
    communicates the method signatures read from the configuration file
    to the IDE, which now has all information to provide proper
    coding assistance.
    Whenever the macro is modified and the program is compiled again,
    the configuration file is rewritten, and the IntelliJ plugin is
    aware of the changes. This is illustrated in in figure \ref{fig:ide-fixed-generic}.

    \begin{figure}[H]
        \begin{center}
            \includegraphics[width=0.8\textwidth]{ide-fixed-generic.png}
            \caption{The IDE is now aware of code changes done by the flyweight macro, which fixes error reporting.}
            \label{fig:ide-fixed-generic}
        \end{center}   
    \end{figure}

    \newpage
    \section{Evaluation}

    In the following sections, we discuss the performance
    impact of introducing the flyweight pattern to Silicon's AST.
    The Silicon implementation without flyweight
    ASTs will be referred to as the base implementation.

    In section \ref{eval-data-structures}, we evaluate the performance difference of using
    different map implementations for the pool holding all
    term instances, seen in listing \ref{lst:flyweight-pattern}, line 7.
    We further discuss the performance impact
    of various other implementation details in the flyweight pattern.
    In section \ref{eval-conclusion}, we present a concluding performance
    evaluation over a wide variety of test cases generated by various frontends.

    \subsection{Performance of Different Data Structures} \label{eval-data-structures}
    
    % \begin{center}
    %     \begin{tabular}{ |l|c|c| } 
    %     \hline
    %     Resources Used & VerCors \\ 
    %     \hline
    %     Number of Parallel Verifiers & 1 \\
    %     \hline
    %     Repetitions & 10 \\ 
    %     \hline
    %     \end{tabular}
    % \end{center}

    The table below shows the performance change of the flyweight implementation
    using different map data structures for the flyweight pool implementation which stores
    term instances. The performance change is relative to the base implementation
    without flyweight pattern.

    \begin{center}
        \begin{tabular}{ ll } 
        \hline
        Data Structure & Relative Performance Change \\ 
        & (Negative is better) \\
        \hline
        \texttt{mutable.HashMap} & -1.3\% \\ 
        \texttt{mutable.WeakHashMap} & -0.2\% \\
        \texttt{concurrent.TrieMap} & -0.2\% \\
        \texttt{concurrent.ListMap} & +89.5\% \\ 
        \hline
        \end{tabular}
    \end{center}

    As expected, the use of \texttt{ListMap} significantly worsens performance,
    as linear time with respect to existing terms is required for a lookup operation.
    The performance of \texttt{HashMap}, \texttt{WeakHashMap} and \texttt{TrieMap}
    are very similar to the base implementation in this benchmark.
    As Silicon may use multiple verifier instances in parallel,
    we chose \texttt{TrieMap} for the concluding performance evaluation in section \ref{eval-conclusion}, as it has
    the additional benefit of being concurrency-safe.

    \subsubsection{Caching Libraries}

    Dedicated maps for caching such as Caffeine \cite{caffeine} where tested as well,
    but they add no advantage over maps
    implemented in the Scala standard library, performance- or otherwise.
    Eviction policies implemented in such caching libraries for example
    add an additional performance overhead, but are unncessary when used
    in our flyweight pool as terms are required to stay in the pool at least as long
    as other references to the term still exist. 

    \subsubsection{Clearing Pools After Each File}

    As an attempt to increase performance, we modified
    the term pool discussed in \ref{eval-data-structures} to be emptied
    after the verification of each file. However no significant
    performance difference could be observed.

    %\subsubsection{AST Reduction for Builtin Equals}

    %\subsubsection{Equality Defining Members}

    %\subsection{IntelliJ Support}
    

    \subsection{Concluding Performance Evaluation} \label{eval-conclusion}

    To analyze the performance difference resulting
    from the flyweight pattern, test programs generated by the VerCors \cite{vercors},
    Prusti \cite{prusti}, Gobra \cite{gobra} and Vyper \cite{vyper} frontends are used
    to emulate real-life usage as close as possible.
    For the benchmark, total verification time  is measured.
    The benchmark is repeated ten times, where the
    slowest and fastest verification times are ignored.
    
    Silicon optionally
    allows parallel verification, however the number of parallel 
    verifiers is set to one. As Scala's \texttt{mutable.TrieMap} is used,
    the flyweight pattern would still work in a parallelized environment.
    
    Figure \ref{fig:absolute-verification-time-part-1} suggests that there is a small
    performance improvement of programs with an absolute verification time greater than
    ten seconds, but for programs with less than ten seconds absolute verification time,
    we observe a performance decrease.
    On Average, the flyweight implementation 2\% was slower,
    which is still within the standard derivation of 2.9\%.

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{performance-change-vs-verification-time-part-1.png}
        \caption{
            Change in performance depending on absolute base verification time. Negative performance difference shows a speedup.
        }
        \label{fig:absolute-verification-time-part-1}
    \end{figure}

    %\subsection{Memory Consumption}

    %\subsection{Memory Pool Hit Rate}

    %\subsection{AST Node Count on Equality Checks}

    \subsection{Why Did Flyweight Fail}

    Although reference equality checks are certainly much faster than recursive structural equality
    checks, changing terms to use the Flyweight pattern didn't result in measurable performance improvements.

    There are some reasons why this might be the case. First, there may be not enough structurally
    equal term instances to justify
    a flyweight pattern. To explore this possibility, we measured the hit percentage
    of looking up terms in the flyweight pool. Remember that on the creation for every
    term, we first check if a term instance with the same component already exists in the
    term pool (listing \ref{lst:flyweight-pattern}, line 10).
    If a term already exists in the pool, we call it a hit, else a miss.
    Many structurally different term instances would lead to
    a low hit percentage, which would render a flyweight pattern inefficient. In our
    benchmarks, a hit percentage of around 83\% was measured, meaning that on average,
    for every term created and added to the flyweight pool,
    only four structurally equal term instances could be avoided.

    Another reason may lie in the depth of the terms on an equality check. If
    the terms are very flat when checking for equality, the additional performance overhead of structural, recursive equality checks becomes negligible compared to reference equality checks, even
    if many equality checks take place. To support this hypothesis, we counted the number of subterms
    contained term at every equality check. For example, if \texttt{Plus(1, Minus(2, 3))} was checked for equality,
    we count 5 contained subterms: once \texttt{Plus}, once \texttt{Minus} and three integer literals.
    Figure \ref{fig:node-count} shows the average subterm count for each term class. On average,
    a term contains around 14 subterms on equality check.

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{node-count.png}
        \caption{Average number of subterms contained in a term instance on equality check.}
        \label{fig:node-count}
    \end{figure}

    To summarize, avoiding on average four structurally equal term instances which contain
    on average 14 subterms is most likely not enough to justify the overhead introduced by the flyweight pattern.


    %\subsection{Automatic Code Generation}

    \subsection{Complementary Benchmarks}

    \subsubsection{Parallelization}

    In this section we discuss the impact of
    parallelization in Silicon.
    Figure \ref{fig:parallelization} illustrates the change of performance
    if parallelization is enabled for the base implementation using 8 threads.

    For small programs, the overhead introduced by parallelization isn't worth
    the speedup, and performance decreases up to 100\%, which isn't much in absolute
    terms as absolute verification times are quite small. For larger programs
    with absolute verification time of five seconds and above, parallelization
    provides a clear performance improvement of up to around 50\%.

    Figure \ref{fig:parallelization-flyweight} shows that using the flyweight
    pattern for terms has no impact on parallelization, as expected.

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{parallelization.png}
        \caption{Change in performance when parallelization is enabled in the base implementation. Negative performance differnece shows a speedup.}
        \label{fig:parallelization}
    \end{figure}

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{parallelization-flyweight.png}
        \caption{Change in performance when parallelization is enabled in the flyweight implementation. Negative performance differnece shows a speedup.}
        \label{fig:parallelization-flyweight}
    \end{figure}


    \subsection{Using Macros to Facilitate Experiments}

    Although the flyweight pattern itself didn't have a significant impact on performance,
    the macro annotation developed to implement the flyweight pattern can
    be quickly modified to perform experiments or benchmarks on the Silicon AST.

    In the following example, the macro is edited to ignore AST simplifications.
    The method \texttt{\_apply} performs AST simplifications. To ignore 
    them, we can avoid calling \texttt{apply} and instead,
    we directly create instances using the \texttt{new} keyword.
    Using the macro, this can be done quickly for all terms by only modifying three lines
    instead of rewriting every term manually.

    \begin{lstlisting}[language=Scala, caption=Use AST simplifications as normal.]
def apply(..$fields) = {
    // ...
    ${
        if (hasRenamedApplyMethod)
            // AST simplifications are potentially performed when
            // creating instances via _apply.
            q"${termName}._apply(..${fieldNames})"
        else
            q"new $className(..${fieldNames})"
    }
    // ...
}  
    \end{lstlisting}

    \begin{lstlisting}[language=Scala, caption=Modified macro to ignore AST simplifications.]
def apply(..$fields) = {
    // ...
    ${
        q"new $className(..${fieldNames})"
    }
    // ...
}  
    \end{lstlisting}

    This illustrates that the macro developed as part of this thesis
    facilitates experiments and benchmarks in Silicon.



    \newpage
    \section{Future Work}

    \begin{itemize}
        \item The macro annotation developed to modify Silicon's Terms
            invites to various experiments. AST simplifications
            for the Silicon AST are concurrently hard-coded in the corresponding \texttt{apply}
            methods of the terms. \texttt{Plus(t, IntLiteral(0))} for example is directly
            simplified to \texttt{t}.
            The usage of a domain-specific language in combination with Scala macros to
            auto-generate AST
            simplifications would allow easy addition and modification
            of AST simplifications in the future. 
        \item In principle, the generic plugin implementation introduced in
            section \ref{full-scala-macro-support} can be used in projects other
            than Silicon which make use of their own macro annotations.
            however, the plugin is not yet fully fleshed out. Concretely,
            the plugin slows down as it reads the many configuration 
            files generated by the macro. This could be avoided by
            using caching techniques, for example. 
            Furthermore, the plugin does not work yet for
            all kind of macro annotations.
            It can be developed
            further to be faster and provide more support for a
            wider range of macro types, providing a valuable addition to
            development of Scala programs using macros in the IntelliJ IDE.
    \end{itemize}

    \newpage
    \part{Joining;\newline Getting Rid of the Branches} \label{part-2}
    \begin{center}
        \vspace{2cm}
        \includegraphics[width=0.9\textwidth]{tree}
    \end{center}

    \newpage
    \section{Approach} \label{p2-approach}

    \subsection{Where to Join}

    For verifying a program, Silicon uses the symbolic execution approach, where the program is interpreted,
    and a symbolic state keeps track of all possible program states at the current point of execution, for all
    possible input values of the program. When encountering certain expressions or statements, for example an
    if-statement, symbolic execution branches with the assumptions of the corresponding program path.
    In the following sections, we list the different statements and expressions 
    that result in a branching of execution paths which can be joined again.

    \subsubsection{Conditional Expressions}

    Consider a conditional expression of the form $Ite(b, e_1, e_2)$.
    As Silicon evaluates this expression, and $b$ cannot definitely be evaluated to true or false,
    two branches are created, the first one assumes that $b$
    is true and the expression is evaluated to be $e_1$.
    the second branch assumes that $b$ is false, and the evaluation yields $e_2$.
    The symbolic execution is continued in both branches.


    \subsubsection{Implications}

    Similar to conditional expressions, symbolic execution branches on implications too. Consider
    an implication of the form $b \implies i$. The first branch assumes that $b$ is true and
    consequently the implication $i$ holds. The second branch assumes that $b$ is false and the
    implication $i$ may or may not hold. 

    \subsubsection{If-Statements} \label{if-statements}

    Viper is parsed into a Control Flow Graph (CFG) consisting of blocks containing
    statements, and edges which connect blocks. Edges can be unconditional or conditional,
    and can potentially form cycles whenever a back edge is connected to a loop head block.
    Symbolic execution branches whenever a block has more than one outgoing edges.

    To join again at the correct location within the CFG after branching,
    the join point for each corresponding branch point has to be identified.
    We introduce a recursive algorithm which maps each branch point to its corresponding join point,
    if it exists:

    \begin{enumerate}
        \item Initialize a queue of blocks to visit and a list of already visited blocks.
            Traverse the CFG in a breath-first way by adding the successors of a block
            to the queue (line 9).
        \item \emph{Base Case.} If a block is visited which already is included in the visited list,
            return this block (line 16) as it is the join point corresponding to the branch point where this procedure was called.
        \item \emph{Recursive Case.} If a block has two outgoing edges, it is a branch point.
            Call this procedure recursively, starting from this branch point (line 13).
    \end{enumerate}

    \begin{lstlisting}[language=Scala, caption={The join point finding algorithm without support for loops.}, label={lst:join-point-algorithm}]
def findJoinPoint(branchPoint)
    queue = branchPoint.successors
    visited = []
    while curr = queue.next
        if curr not in visited
            visited += curr
            match curr.successors
                case [next]:
                    queue += next
                // Branch point found,
                // find corresponding join point recursively.
                case [_, _]:
                    queue += findJoinPoint(curr)
        else
            // curr is the join point for this branch point.
            return curr

    
        
    \end{lstlisting}

    Special attention has to be paid to loops. If our algorithm follows a back edge before finding a join point,
    it may do the recursion again for the same branch point, leading to non-termination.
    To avoid this, all already visited loop head blocks
    are remembered for later recursive invocations. Already visited loop head blocks are not followed again.
    Whenever a join point exists for a branch point created via if-statement, we are now able
    to join the branches.

    Branches created by both conditional expressions and implications are already being joined if they are pure,
    that is they do not modify the symbolic state.
    Branches resulting from impure conditional expressions and implications,
    and from all if-statements aren't joined again, meaning that
    all statements later down the verification path are evaluated twice. Both of these
    verification paths may branch again, eventually leading to exponential growth in branches.
    These branches are avoided when joining the symbolic execution paths back together.
    However, joining execution paths requires merging the symbolic state, which
    ultimately produces more complex symbolic state entries.

    Intuitively, the same work has to be done with or without joining. The difference is that
    no joining leads
    to many execution paths with simpler symbolic states, and thus more in quantity, but less complex
    invocations of the SMT solver. More joining on the other hand leads to
    fewer execution paths but with more complex symbolic states, resulting in fewer,
    but more complex invocations of the SMT solver.


    \subsection{Merging the Symbolic State} \label{merging-the-symbolic-state}
    
    After finding the appropriate locations for joining, the information
    gathered through both branches in the symbolic state have to be merged
    into a single symbolic state to continue symbolic execution in a single
    path.

    To formalize the merging process, we define
    a symbolic state $\sigma$ of type $\Sigma := (\Gamma, \Pi, H)$. The entries
    defined as follows:

    \begin{itemize}
        \item A store $\gamma$ of type $\Gamma := Var \to V$ maps local variables to their symbolic values.
        \item A path condition stack $\pi$ of type $\Pi$ records all assumptions that have been made in the current verification path.
        \item A symbolic heap $h$ of type $H$ that records which locations are accessible and their respective symbolic values.
    \end{itemize}

    For the following subsections, assume that after the verification branched under the condition $c$ of type $Bool$,
    two symbolic states $\sigma_1 = (\gamma_1, \pi_1, h_1)$ under the branch condition $c = c_1$, and
    $\sigma_2 = (\gamma_2, \pi_2, h_2)$ under the branch condition $\overline{c} = c_2$ are to be merged,
    resulting in the new state $\sigma_3 = (\gamma_3, \pi_3, h_3)$.

    Note that this core idea could be extended to merge more than two states at once. In practice however,
    no more than two states are merged at once.

    \subsubsection{Merging the Store}

    For merging stores $\gamma_1$ and $\gamma_2$, we consider two cases:

    \begin{enumerate}
        \item For some local variable $x$, we have $x \mapsto v_1 \in \gamma_1$ and $x \mapsto v_2 \notin \gamma_2$.
            In this case, we can simply omit $x$ in the new store $\gamma_3$ as we can assume that $x$ won't
            be needed later down the verification path.
        \item For some local variable $x$, we have $x \mapsto v_1 \in \gamma_1$ and $x \mapsto v_2 \in \gamma_2$.
            In this case, we modify the heap chunk such that $x \mapsto Ite(c_1, v_1, v_2) \in \gamma_3$.
    \end{enumerate}

    \subsubsection{Merging the Heap} \label{merging-the-heap}

    A heap is essentially a collection of heap chunks, where each heap chunk provides information
    about the location's value and the receiver's permission amount to the location.
    As there may be multiple heap chunks making statements about the same location,
    Silicon provides a mechanism to merge them using a mechanism called state consolidation.
    To merge heaps $h_1$ and $h_2$, we perform the following steps:

    \begin{enumerate}
        \item Every non-quantified heap chunk $c$ for which $c \in h_1$ and $c \in h_2$ holds can be carried over to $h_3$
            without modifications.
        \item Non-quantified heap chunks $c := x.f \mapsto t \perm p$ where $c \in h_1$ and $c \notin h_2$ are modified to
            have permissions only if $c_1$ holds: $c' := x.f \mapsto t \perm Ite(c_1, p, 0) \in h_3$
        \item Finally, $h_3$ can be consolidated to avoid multiple aliasing heap chunks.
    \end{enumerate}

    Quantified heap chunks are of the shape $\forall x: c(x) \implies e(x).f \mapsto v(x) \perm p(x)$.
    In practice, they are rewritten to the equivalent form $\forall x: e(x).f \mapsto v(x) \perm p'(x)$,
    where $p'(x)$ is defined as:

    \begin{math}
        p'(x) = \begin{cases}
            p(x) & \text{if } c(x) \text{ is true,} \\
            0 & \text{else}
        \end{cases}
    \end{math}

    Analogously to non-quantified heap chunks, we can simply modify the permission amounts of
    quantified heap chunks to $p''(x) = Ite(c_1, p'(x), 0)$.

    \subsubsection{Merging the Path Conditions}

    For path conditions, the functionality for merging is already provided.
    This is done by putting the collected path conditions of each branch
    under an implication with the corresponding branch condition.

    \newpage
    \section{Implementation}

    \subsection{Finding Join Points}

    To find join points within the CFG, the algorithm described in listing \ref{lst:join-point-algorithm} is implemented,
    but with some additional modifications to support loops. All loop heads that
    were already seen are remembered to avoid infinite recursion issues as described
    in \ref{if-statements}.
    The algorithm produces a mapping from each branch point to the respective join point.
    If an if-statement is encountered during verification, we check if a 
    corresponding join point exists and the branches can be joined again.

    \subsection{Implementing State Merges} \label{implementing-state-merges}

    Store and heap merges are implemented as according to section \ref{merging-the-symbolic-state}.
    Silicon's state however consists of some more fields carrying additional data, that have to be merged with caution.
    Some of these additional fields are chages that are used at various times during verification
    to increase performance.
    Such caches within the state currently are emptied instead of merged for simplicity.

    Our implementation can be optionally enabled by passing the command-line
    argument \texttt{--moreJoins} to the Silicon executable. To illustrate the difference
    in verification, consider the Viper program in listing \ref{lst:viper-example}.
    Table \ref{execution-1} shows the execution trace of Silicon with more joins disabled,
    in table \ref{execution-2}, more joins is enabled.

    \begin{lstlisting}[language=Scala, caption={An example Viper program.}, label={lst:viper-example}]
method test(b: Bool) {
    var x: Int := 0
    if (b) {
        x := x + 5
    } else {
        x := x + 7
    }
    x := x + 3
    assert x <= 10
}       
    \end{lstlisting}

    \begin{center}
        \begin{tabular}{ l|l|l }
            Operation & Store & Path Conditions \\
            \hline
            \texttt{var x: Int := 0} & & \\
            \texttt{x := x + 5} & $x \mapsto 0$ & $b$ \\
            \texttt{x := x + 3} & $x \mapsto 5$ & $b$ \\
            \texttt{assert x <= 10} & $x \mapsto 8$ & $b$ \\
            \texttt{x := x + 7} & $x \mapsto 0$ & $!b$ \\
            \texttt{x := x + 3} & $x \mapsto 7$ & $!b$ \\
            \texttt{assert x <= 10} & $x \mapsto 10$ & $!b$ \\
        \end{tabular}
        \captionof{table}{Symbolic execution trace of viper program \ref{lst:viper-example} using the base implementation without joining.}
        \label{execution-1}
    \end{center}

    \begin{center}
        \begin{tabular}{ l|l|l }
            Operation & Store & Path Conditions \\
            \hline
            \texttt{var x: Int := 0} & & \\
            \texttt{x := x + 5} & $x \mapsto 0$ & $b$ \\
            \texttt{x := x + 7} & $x \mapsto 0$ & $!b$ \\
            \texttt{x := x + 3} & $x \mapsto Ite(b, 5, 7)$ & \\
            \texttt{assert x <= 10} & $x \mapsto Ite(b, 5, 7) + 3$ & \\
        \end{tabular}
        \captionof{table}{
            Symbolic execution trace with joining.
            Joining leads to less execution steps,
            but to a more complex state.
        }
        \label{execution-2}
    \end{center}

    \subsection{Configuration and Command Line Arguments}

    Our heap merging implementation may
    introduce multiple heap chunks making statements about the same location. This can be
    avoided if the state resulting from the merging process consolidated as
    described in section \ref{merging-the-heap}. But doing a state consolidation
    after every merge has shown to decrease performance.
    Instead, we avoid state consolidation after each merge alltogether.

    Proving assertions or permission amounts of a location in the heap
    is normally done greedily, where the heap chuncks are
    traversed until the first matching heap chunk for the location is found.
    This greedy algorithm does not work if multiple heap chunks for the same location
    exist. To avoid any issues,
    option \texttt{--enableMoreCompleteExhale} should be used,
    which enables a more sophisticated method of finding matching heap chunks.
    Furthermore, more complete exhale provides a small performance improvement
    by itself, as later discussed in section \ref{more-complete-exhale}.

    As state merging currently empties caches held in the state
    instead of merging them, the option
    \texttt{--disableCaches} which disables the caches
    was also added to Silicon in order to provide more fair benchmarks.

    \newpage
    \section{Evaluation}

    In the following section \ref{concluding-performance-evaluation}, we analyze the performance difference
    of our implementation using more joins, compared to the base implementation
    which only joins on simple cases, as described \ref{p2-approach}. in Additionally, we provide some complementary benchmark results
    which were a byproduct of this thesis. 


    \subsection{Concluding Performance Evaluation} \label{concluding-performance-evaluation}
    
    The benchmark uses frontend-generated Viper programs from Vercors \cite{vercors},
    Prusti \cite{prusti}, Gobra \cite{gobra}, Vyper \cite{vyper} and Nagini \cite{nagini}
    to emulate real-life usage as close as possible, and
    the total duration of each verification run is measured.
    The benchmark is repeated five times, where the
    slowest and fastest verification times are ignored.
    The results show that verification time
    decreases by around 3\% on average when using more joins,
    relative to a version which doesn't
    make use of the implemented joining procedures.
    Intuitively, one would expect that fewer branches lead to better
    performance, however state merging introduces a more complex final state which again tends to worsen
    performance.
    
    When comparing the number of state merges 
    to the verification time performance difference, no clear correlation is
    visible, as can be seen in figure \ref{fig:state-merges}.

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{state-merges-vs-performance.png}
        \caption{
            Impact on the number of state merges on the performance,
            negative performance difference shows a speedup.
        }
        \label{fig:state-merges}
    \end{figure}

    Interestingly, figure \ref{fig:absolute-verification-time} shows that the performance seems to improve about 3.3\% for programs
    with an absolute base verification time of up to 0.5 seconds. With increasing verification
    time, the performance decreases, and for programs with a verification time greater
    than 10 seconds, we get a decrease of performance of nearly 25\%.
    
    This observation suggests that for smaller programs where fewer joins are needed, the more complex symbolic
    states caused by joining is worth trading for the benefit of having fewer branches. For
    larger programs, the symbolic state may become overly complex up to a point that the advantage of
    fewer branches no longer pays off.

    \begin{figure}[H]
        \includegraphics[width=\linewidth]{performance-change-vs-verification-time-part-2.png}
        \caption{
            Change in performance depending on absolute base verification time. Negative performance difference shows a speedup.
        }
        \label{fig:absolute-verification-time}
    \end{figure}

    \subsection{Complementary Benchmarks}

    \subsubsection{Disable Caching} \label{disable-caching}

    As state merging currently empties caches instead of merging them, an option to disable the caches
    entirely was added to Silicon in order to provide more fair benchmarks. Disabling caches results in a performance decrease of 1.9\%
    compared to the base implementation with caching enabled.

    \subsubsection{More Complete Exhale} \label{more-complete-exhale}

    Silicon additionally provides an option of enabling a more complete version of exhaling permissions,
    which must be used when joining is enabled. This is because joining may result in permissions of a 
    location being divided into multiple heap chunks. Benchmarks have shown that enabling more complete 
    exhale results in a performance increase of 2.9\%.

    \newpage
    \section{Future Work}

    \begin{itemize}
        \item Currently, we use conditional expressions for merging both
            the heap and the store. Merging can also be done by introducing
            new symbolic variables and conditionalizing them via implications
            in the path conditions. For example, if the store is merged to
            $v = Ite(b, e_1, e_2)$, we could analogously express this using
            a new symbolic variable $v'$ as
            $v = v'$ and restrict the value of $v'$ in the path conditions using implications
            $b \implies v' = e_2$ and $\overline{b} \implies v' = e_2$.
        \item Section \ref{implementing-state-merges} discusses how caches are
            used in Silicons state. For simplicity, we empty such chaches
            instead of merging them. Implementing sophisticated cache merges would most
            certainly result in a performance increase when using more joins.
    \end{itemize}

    \newpage
    %\bibliographystyle{unsrt}
    \printbibliography
    
\end{document}